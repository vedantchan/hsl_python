{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "import scipy.signal as sig\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import romb\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "import glob\n",
    "from scipy.signal import medfilt,butter\n",
    "import scipy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "\n",
    "def num_to_epoch(epoch_number):\n",
    "    if epoch_number == 0:\n",
    "        return 'UP1'\n",
    "    elif epoch_number == 1:\n",
    "        return 'UP2'\n",
    "    elif epoch_number == 2:\n",
    "        return 'P1'\n",
    "    elif epoch_number == 3:\n",
    "        return 'P2'\n",
    "    elif epoch_number == 4:\n",
    "        return 'REC'\n",
    "    else:\n",
    "        raise Exception('Invalid Epoch Number')\n",
    "\n",
    "def num_to_subject(subject_number):\n",
    "    return 'Subject'+str(subject_number)\n",
    "\n",
    "def get_signal(measure, epoch, subjno):\n",
    "    if isinstance(epoch,str):\n",
    "        return np.ravel(df.loc[measure, epoch][num_to_subject(subjno)])\n",
    "    elif isinstance(epoch, int):\n",
    "        return np.ravel(df.loc[measure, num_to_epoch(epoch)][num_to_subject(subjno)])\n",
    "\n",
    "def get_split_signal(measure, epoch, subjno, splitno):\n",
    "    if isinstance(epoch,str):\n",
    "        return np.array_split(np.ravel(df.loc[measure, epoch][num_to_subject(subjno)]), n_splits)[splitno]\n",
    "    elif isinstance(epoch, int):\n",
    "        return np.array_split(np.ravel(df.loc[measure, num_to_epoch(epoch)][num_to_subject(subjno)]),n_splits)[splitno]\n",
    "    \n",
    "def full_signal(measure, subjno):\n",
    "    return np.concatenate((\n",
    "        np.ravel(df.loc[measure, num_to_epoch(0)][num_to_subject(subjno)]),\n",
    "        np.ravel(df.loc[measure, num_to_epoch(1)][num_to_subject(subjno)]),\n",
    "        np.ravel(df.loc[measure, num_to_epoch(2)][num_to_subject(subjno)]),\n",
    "        np.ravel(df.loc[measure, num_to_epoch(3)][num_to_subject(subjno)]),\n",
    "        np.ravel(df.loc[measure, num_to_epoch(4)][num_to_subject(subjno)]),\n",
    "    ))\n",
    "\n",
    "def is_perturbed(epoch):\n",
    "    if epoch == 0 or epoch == 1 or epoch == 4:\n",
    "        return 0\n",
    "    elif epoch == 2 or epoch == 3:\n",
    "        return 1\n",
    "    \n",
    "def scaled_correlation_time(signal1, signal2):\n",
    "    signal1 = (signal1 - np.mean(signal1))/np.std(signal1)\n",
    "    signal2 = (signal2 - np.mean(signal2))/np.std(signal2)\n",
    "    acorr = np.correlate(signal1, signal2, mode='full')\n",
    "    acorr = acorr[(acorr.size // 2 ):] / np.max(acorr)\n",
    "#     plt.plot(acorr)\n",
    "    tau = np.argmax([acorr < 1/np.exp(1)])\n",
    "    return tau / len(acorr)\n",
    "\n",
    "def correlation_integral(signal1, signal2):\n",
    "    signal1 = (signal1 - np.mean(signal1))/np.std(signal1)\n",
    "    signal2 = (signal2 - np.mean(signal2))/np.std(signal2)\n",
    "    acorr = np.correlate(signal1, signal2, mode='full')\n",
    "    acorr = acorr[(acorr.size // 2 ):] / np.max(acorr)\n",
    "#     plt.plot(acorr)\n",
    "    integral = np.trapz(acorr)\n",
    "    return integral\n",
    "\n",
    "def plot_full_experiment(measure, subjno):\n",
    "    s1 = np.ravel(df.loc[measure, num_to_epoch(0)][num_to_subject(subjno)])\n",
    "    s2 = np.ravel(df.loc[measure, num_to_epoch(1)][num_to_subject(subjno)])\n",
    "    s3 = np.ravel(df.loc[measure, num_to_epoch(2)][num_to_subject(subjno)])\n",
    "    s4 = np.ravel(df.loc[measure, num_to_epoch(3)][num_to_subject(subjno)])\n",
    "    s5 = np.ravel(df.loc[measure, num_to_epoch(4)][num_to_subject(subjno)])\n",
    "    fullsignal = np.concatenate((s1,s2,s3,s4,s5))\n",
    "    plt.plot(fullsignal,'k',label = measure)\n",
    "    plt.axvline(x = len(s1), color = 'k', linestyle = '--')\n",
    "    plt.axvline(x = len(s1)+len(s2), color = 'k', linestyle = '--')\n",
    "    plt.axvline(x = len(s1)+len(s2)+len(s3), color = 'k', linestyle = '--')\n",
    "    plt.axvline(x = len(s1)+len(s2)+len(s3)+len(s4), color = 'k', linestyle = '--')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shimfiles = glob.glob('shimmerData/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shimmer(subjno, part, epochno):\n",
    "    epoch = num_to_epoch(epochno)\n",
    "    if epoch == 'REC':\n",
    "        epoch = 'Rec'\n",
    "    elif epoch == 'P1':\n",
    "        epoch = '_P1'\n",
    "    elif epoch == 'P2':\n",
    "        epoch = '_P2'\n",
    "    \n",
    "    for file in shimfiles:\n",
    "        if str(subjno) in file and part in file and epoch in file:\n",
    "            data = pd.read_csv(file, header = None)\n",
    "    try:\n",
    "        vectors = np.asarray(data[[1,2,3]])\n",
    "    except:\n",
    "        print('mising shimmer data. skipping...')\n",
    "        return np.repeat(np.nan, 1000)\n",
    "    mean = np.mean(vectors, axis = 0)\n",
    "    std = np.std(vectors, axis = 0)\n",
    "    \n",
    "    vectors = (vectors - mean[np.newaxis,:])\n",
    "\n",
    "    #plt.plot(vectors)\n",
    "    \n",
    "    z_vector = np.linalg.norm(vectors, axis = 1)\n",
    "\n",
    "    return z_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_shimmer(subjno, part, epochno, splitno):\n",
    "\n",
    "    z_vector = get_shimmer(subjno,part,epochno)\n",
    "\n",
    "    return np.array_split(z_vector,n_splits)[splitno]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_statistics(signal):\n",
    "    mean = np.mean(signal)\n",
    "    std = np.std(signal)\n",
    "    skewness = stats.skew(signal)\n",
    "    kurtosis = stats.kurtosis(signal)\n",
    "    maximum = np.max(signal)\n",
    "    minimum = np.min(signal)\n",
    "    iqr = stats.iqr(signal)\n",
    "    variation = stats.variation(signal)\n",
    "    entropy = stats.entropy(np.abs(signal))\n",
    "    corrtime = scaled_correlation_time(signal,signal)        \n",
    "    return np.asarray([mean, std, skewness, kurtosis, maximum, minimum, iqr, variation, entropy, corrtime])\n",
    "\n",
    "def spectrum_statistics(signal):\n",
    "    \n",
    "    fs,pxx = sig.periodogram(signal, fs = 50, nfft = 1000, scaling = 'density', detrend = 'constant')\n",
    "    \n",
    "#     plt.plot(fs,pxx)\n",
    "    #plt.xlim(0,0.1)\n",
    "\n",
    "    peak = fs[np.argmax(pxx)]\n",
    "    peakmag = np.max(pxx)\n",
    "    integral = np.trapz(pxx,fs)\n",
    "    energy = np.dot(pxx,pxx)\n",
    "    shannon = np.sum(pxx*np.log(1/pxx))\n",
    "\n",
    "    # Add wavelet analysis\n",
    "\n",
    "    return [peak, peakmag, integral, energy, shannon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signal = get_shimmer(42, 'body', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2766343281788908"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_dists(signal, m):\n",
    "    count = 0\n",
    "    N = len(signal)\n",
    "    max_dists = []\n",
    "    while count < N-m-1:\n",
    "        x_i = signal[count:count+m-1]\n",
    "        x_j = signal[count+1:count+m]\n",
    "        max_dists.append(scipy.spatial.distance.chebyshev(x_i, x_j))\n",
    "        count += 1\n",
    "    return max_dists\n",
    "\n",
    "\n",
    "def approx_entropy(signal):\n",
    "    N = len(signal)\n",
    "    m = 2 #or 3 -- dimensionality? \n",
    "    r = 0.3\n",
    "        \n",
    "    \n",
    "    def phi(m):\n",
    "        d_func = max_dists(signal,m)\n",
    "        C = []\n",
    "        for i in d_func:\n",
    "            if i <= r and i > 0:\n",
    "                C.append(i/(N-m+1)) \n",
    "        return (N - m + 1.0)**(-1) * sum(np.log10(C))\n",
    "    return (abs(phi(m)-phi(m+1)))\n",
    "\n",
    "\n",
    "approx_entropy(test_signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009862868702608938\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sample_entropy(signal):\n",
    "    N = len(signal)\n",
    "    m = 2\n",
    "    r = 0.2\n",
    "    \n",
    "    A_list = max_dists(signal,m+1)\n",
    "    B_list = max_dists(signal,m)\n",
    "    \n",
    "    count_A = 0\n",
    "    count_B = 0\n",
    "    for i in A_list:\n",
    "        if i < r:\n",
    "            count_A += 1\n",
    "    for i in B_list: \n",
    "        if i < r:\n",
    "            count_B += 1\n",
    "    return math.log10(count_A/count_B)*-1\n",
    "\n",
    "print(sample_entropy(test_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024643562693091963\n"
     ]
    }
   ],
   "source": [
    "def multiscale_entropy(signal):\n",
    "    mses = []\n",
    "    for T in range(2,20,2):\n",
    "        coarse_grain = []\n",
    "        i = 0\n",
    "        while i < len(signal):\n",
    "            new_val = sum(signal[i:i+T])/T\n",
    "            coarse_grain.append(new_val)\n",
    "            i += T\n",
    "        mses.append(sample_entropy(coarse_grain))\n",
    "    return np.mean(mses)\n",
    "        \n",
    "print(multiscale_entropy(test_signal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_subjects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4d471a02cd46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msubjno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_subjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msplitno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_subjects' is not defined"
     ]
    }
   ],
   "source": [
    "bfeatures = [];\n",
    "n_splits = 50\n",
    "targets = [];\n",
    "allfeatures = []\n",
    "hfeatures = [];\n",
    "\n",
    "for subjno in tqdm(all_subjects[8:]):\n",
    "    for epoch in range(4):\n",
    "        for splitno in range(n_splits):\n",
    "            \n",
    "            bshim_signal = get_split_shimmer(subjno,'body',epoch,splitno)\n",
    "            bfeature = np.concatenate((signal_statistics(bshim_signal), spectrum_statistics(bshim_signal)))\n",
    "            bfeatures.append(bfeature)\n",
    "            \n",
    "            hshim_signal = get_split_shimmer(subjno,'head',epoch,splitno)\n",
    "            hfeature = np.concatenate((signal_statistics(hshim_signal), spectrum_statistics(hshim_signal)))\n",
    "            hfeatures.append(hfeature)\n",
    "\n",
    "            targets.append(is_perturbed(epoch))\n",
    "\n",
    "            allfeature = np.concatenate((bfeature,hfeature))\n",
    "            allfeatures.append(allfeature)\n",
    "        \n",
    "\n",
    "allfeature_names = ['bodyshim_mean', 'bodyshim_std', 'bodyshim_skewness', 'bodyshim_kurtosis', \\\n",
    "                    'bodyshim_maximum', 'bodyshim_minimum', \\\n",
    "                            'bodyshim_iqr', 'bodyshim_variation', 'bodyshim_entropy', 'bodyshim_corrtime',\\\n",
    "                       'bodyshim_peakfreq','bodyshim_peakpower','bodyshim_powerint','bodyshim_specenergy',\\\n",
    "                   'bodyshim_shannon', \\\n",
    "                   'headshim_mean', 'headshim_std', 'headshim_skewness', 'headshim_kurtosis', \\\n",
    "                    'headshim_maximum', 'headshim_minimum', \\\n",
    "                            'headshim_iqr', 'headshim_variation', 'headshim_entropy', 'headshim_corrtime',\\\n",
    "                       'headshim_peakfreq','headshim_peakpower','headshim_powerint','headshim_specenergy',\\\n",
    "                   'headshim_shannon']\n",
    "fdf = pd.DataFrame(allfeatures, columns = allfeature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'allfeature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-39ede99fe753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'allfeature_names' is not defined"
     ]
    }
   ],
   "source": [
    "targets = np.asarray(targets)\n",
    "fdf = pd.DataFrame(allfeatures, columns = allfeature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2456875bb99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwhere\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fdf' is not defined"
     ]
    }
   ],
   "source": [
    "where = np.asarray(np.sum(np.isnan(fdf),1)) > 0\n",
    "X = np.asarray(fdf.loc[~where])\n",
    "t = np.asarray(targets)[~where]\n",
    "\n",
    "\n",
    "sc = preprocessing.MinMaxScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "scores = mutual_info_classif(X_scaled,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5d7a5ced7a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msortidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msortidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msortidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mutual Information Feature Selection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sortidx = np.argsort(-scores)\n",
    "plt.bar(fdf.columns[sortidx],height=scores[sortidx],)\n",
    "plt.xticks(rotation=45, ha='right',fontsize=15);\n",
    "plt.title('Mutual Information Feature Selection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
